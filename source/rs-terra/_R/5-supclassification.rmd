# Supervised Classification

```{r setup, echo=TRUE, include=FALSE}
library(knitr)
library(terra)
```

Here we explore supervised classification for a simple land use land cover (LULC) mapping task. Various supervised classification algorithms exist, and the choice of algorithm can affect the results. Here we explore two related algorithms (CART and RandomForest).

In supervised classification, we have prior knowledge about some of the land-cover types through, for example, fieldwork, reference spatial data or interpretation of high resolution imagery (such as available on Google maps). Specific sites in the study area that represent homogeneous examples of these known land-cover types are identified. These areas are commonly referred to as training sites because the spectral properties of these sites are used to train the classification algorithm. 

The following examples uses a Classification and Regression Trees (CART) classifier (Breiman et al. 1984) ([further reading](https://doi.org/10.1016/S0034-4257(97)00049-7) to predict land use land cover classes in the study area.

We will perform the following steps:  
  
* Create sample sites used for classification  
* Extract cell values from Landsat data for the sample sites  
* Train the classifier using training samples  
* Classify the Landsat data using the trained model  
* Evaluate the accuracy of the model 
   

## Landsat data to classify
Here is our Landsat data.

```{r landsat}
library(terra)

# We read the 6 bands from the Landsat image we previously used
raslist <- paste0('data/rs/LC08_044034_20170614_B', 2:7, ".tif")
landsat <- rast(raslist)
names(landsat) <- c('blue', 'green', 'red', 'NIR', 'SWIR1', 'SWIR2')
```

  
## Reference data

Training and/or validation data can come from a variety of sources. In this example, we use some training polygons we have already collected from other sources. We have already used this for making the spectral plots.There are 5 distinct classes -- built,cropland,fallow,open and, water and we hope to find the pixels under this categroies based on our knowledge of training sites.

``` {r samples}
library(sp)

# load polygons with land use land cover information
samp <- readRDS('data/rs/samples.rds')
#samp <- spTransform(samp, crs(landsat5))

# check the distribution of the polygons
plot(samp)
text(samp, samp$class)
```

Note that the polygons are [`SpatialPolygonsDataFrame` object](https://www.rdocumentation.org/packages/sp/versions/1.3-1/topics/SpatialPolygonsDataFrame-class). Next we generate random points within each polygons and convert those to `SpatVector` object.

```{r sample-points}
set.seed(1)
# generate point samples from the polygons 
ptsamp <- spsample(samp, 1000, type='random')

# add the land cover class to the points
ptsamp$class <- over(ptsamp, samp)$class

# We convert `ptsamp` to `SpatVector`
ptsamp <- vect(ptsamp)
```

Alternatively, we can generate the training and validation sample sites using a reference land use land cover data. For example, the [National Land Cover Database 2011 (NLCD 2011)](https://www.mrlc.gov/nlcd2011.php) is a land cover product for the United States. NLCD is a 30-m Landsat-based land cover database spanning 4 epochs (1992, 2001, 2006 and 2011). NLCD 2011 is based primarily on a decision-tree classification of circa 2011 Landsat data.

Detailes of the class mapped in NCLD 2011 can be found here (here)[https://www.mrlc.gov/nlcd11_leg.php]. It has two pairs of class values and names that correspond to the levels of land use and land cover classification system. These levels usually represent the level of complexity, level I being the simplest with broad land use land cover categories. Read [this report by Anderson et al](https://pubs.usgs.gov/pp/0964/report.pdf) to learn more about this land use and land cover classification system. 

```{r nlcd, echo = FALSE, include = FALSE, eval = FALSE}
# Robert: Hide this entire section for now

nlcd <- rast('data/rs/nlcd-L1.tif')
names(nlcd) <- c("nlcd2001", "nlcd2011")

# The class names and colors for plotting
nlcdclass <- c("Water", "Developed", "Barren", "Forest", "Shrubland", "Herbaceous", "Planted/Cultivated", "Wetlands")
classdf <- data.frame(classvalue1 = c(1,2,3,4,5,7,8,9), classnames1 = nlcdclass) 

# Hex codes of colors
classcolor <- c("#5475A8", "#B50000", "#D2CDC0", "#38814E", "#AF963C", "#D1D182", "#FBF65D", "#C8E6F8") 

nlcd2011 <- nlcd[[2]]

# Robert: how can we change legend labels in terra::plot? related to RAT discussion we had.

# plot the locations over the original nlcd raster
plot(nlcd2011) #, main = 'NLCD 2011', col = classcolor, add = TRUE)
points(ptsamp) # hides previous plot

# generate sample sites from NLCD raster
# Set the random number generator to reproduce the results
set.seed(99)

# Sampling
samp2011 <- sampleRegular(nlcd2011, size = 500)

samp2011val <- values(samp2011)
# Number of samples in each class
table(samp2011val)

# Robert: to get coordinates of the sampled cell, use as.points or as.data.frame(x[[1]], xy=TRUE, cells=TRUE)? required as sampleRegular/Random returns SpatRaster

```

## Extract spectral values for the training sites

Once we have the training sites, we can extract the cell values from `landsat` layers. These band values will be the predictor variables and "class" from `ptsamp` will be the response variable.

```{r extractvalues}
# We use the x-y coordinates to extract the spectral values for the locations 
xy <- as.matrix(geom(ptsamp)[,c('x','y')])

df <- extract(landsat, xy)

# Quick check for the extracted values
head(df)

# combine lulc class information with extracted values
sampdata <- data.frame(class = ptsamp$class, df)
```

We often find classnames are provided as string labels (e.g. water, crop, vegetation) that need to be 'relabelled' to integer or factors if only string labels are supplied before using them as response variable in the classification. There are several approaches that could be used to convert these classes to integer codes. We can make a function that will reclassify the character strings representing land cover classes into integers based on the existing factor levels.

## Train the classifier 

Now we will train the classification algorithm using `sampdata` dataset. 

```{r cart-train}
library(rpart) 

# Train the model
cartmodel <- rpart(as.factor(class)~., data = sampdata, method = 'class', minsplit = 5)

```

One of the primary reasons behind choosing `cart` model is to have a closer look at the classification model. Unlike other models, `cart` provides very simple way of inspecting and plotting the model structure. 

``` {r cart-plot, fig.height=6, fig.width=6}
# print trained model
print(cartmodel)

# Plot the trained classification tree
plot(cartmodel, uniform=TRUE, main="Classification Tree")
text(cartmodel, cex = 1)
```

See `?rpart.control` to set different parameters for building the model.

You can print/plot more about the `cartmodel` created in the previous example. E.g. you can use `plotcp(cartmodel)` to learn about the cost-complexity (`cp` argument in `rpart`). 

## Classify

Now we have our trained classification model (`cartmodel`), we can use it to make predictions, that is, to classify all cells in the `landsat5` RasterStack.

**Important** The layer names in the Raster object to be predicted should exactly match those expected by the trained model. This will be the case if the same Raster object was used (via extract) to obtain the values to fit the model. Otherwise you need to specify the matching names.

```{r prediction}
# Now predict the subset data based on the model; prediction for entire area takes longer time
classified <- predict(landsat, cartmodel, na.rm = TRUE)
classified
plot(classified)
```

Observe that there are `r nlyr(classified)` layers in the `classified` object, each of the layer representing the probability of a particular LULC class. We assign each pixel to the LULC class with highest probability. Next we learn how to combine them to create the final single layer product. 

To plot the final classification result, we first specify the colors corresponding to each lund use cateogry. In the plot classvalues are printed.

```{r combine-results, fig.width=8, fig.height=8}

class <- c("built","cropland","fallow","open","water")
mycolor <- c('darkred', 'yellow', 'burlywood', 'cyan', 'blue')

classdf <- data.frame(classvalue = c(1,2,3,4,5), 
                      classnames = class,
                      color = mycolor)

lulc <- app(classified, fun = which.max)

lulcc <- as.factor(lulc)
levels(lulcc) <- as.character(classdf$classnames)
plot(lulcc)
```

If not satisfied with the results, you can select more samples and use additional predictor variables to improve the classification results. The choice of classifier also plays an important role. Next we show how to test the performance the classification model.

## Model evaluation

This section discusses how to assess the accuracy of the model to get an idea of how accurate the classified map might be. Two widely used measures in remote sensing are "overall accuracy" and "kappa". You can perform the accuracy assessment using the independent samples.

To evaluate any model, you can use k-fold cross-validation (you can also do single-fold). In this technique the data used to fit the model is split into `k` groups (typically 5 groups). In turn, one of the groups will be used for model testing, while the rest of the data is used for model training (fitting).

```{r kfold-setup}
set.seed(99)

# number of folds
k <- 5
j <- sample(rep(1:k, each = round(nrow(sampdata))/k))

# or using `dismo` library to get equal number of samples 
# library(dismo)
# j <- kfold(sampdata, k = 5, by=sampdata$classvalue)

table(j)
```

Now we train and test the model five times, each time computing the predictions and storing that with the actual values in a list. Later we use the list to compute the final accuarcy.

```{r k-fold}

x <- list()

for (k in 1:5) {
	train <- sampdata[j!= k, ]
	test <- sampdata[j == k, ]
	cart <- rpart(as.factor(class)~., data=train, method = 'class', 
	              minsplit = 5)
	pclass <- predict(cart, test, na.rm = TRUE)
	# assign class to maximum probablity
	pclass <- apply(pclass, 1, which.max)
	# create a data.frame using the reference and prediction
	x[[k]] <- cbind(test$class, as.integer(pclass))
}
```

Now combine the five list elements into a single data.frame, using `do.call` and compute a confusion matrix.

```{r confusion-matrix}
y <- do.call(rbind, x)
y <- data.frame(y)
colnames(y) <- c('observed', 'predicted')

# confusion matrix
conmat <- table(y)
# change the name of the classes
colnames(conmat) <- classdf$classnames
rownames(conmat) <- classdf$classnames
print(conmat)
```

__Question 1__:*Comment on the miss-classification between different classes.*

__Question 2__:*Can you think of ways to to improve the accuracy.*


Compute the overall accuracy and the "kappa" statistic. 

Overall accuracy:

```{r overallaccuracy}
# number of total cases/samples
n <- sum(conmat) 
n

# number of correctly classified cases per class
diag <- diag(conmat) 

# Overall Accuracy
OA <- sum(diag) / n
OA
```

Kappa:

```{r kappa}
# observed (true) cases per class
rowsums <- apply(conmat, 1, sum) 
p <- rowsums / n 

# predicted cases per class
colsums <- apply(conmat, 2, sum) 
q <- colsums / n 

expAccuracy <- sum(p*q)
kappa <- (OA - expAccuracy) / (1 - expAccuracy)
kappa
```

Producer and user accuracy

```{r User/Producer accuracy}
# Producer accuracy
PA <- diag / colsums

# User accuracy
UA <- diag / rowsums

outAcc <- data.frame(producerAccuracy = PA, userAccuracy = UA)
outAcc
```

__Question 3__:*Perform the classification using Random Forest classifiers from the `randomForest` package*


__Question 4__:*Plot the results of rpart and Random Forest classifier side-by-side.*


__Question 5 (optional)__:*Repeat the steps for other years using Random Forest*. For example you can use the cloud-free composite image `data/centralvalley-2001LE7.tif`. This data is collected by the [Landsat 7](https://landsat.gsfc.nasa.gov/landsat-7/) platform. You can use the [National Land Cover Database 2001 (NLCD 2001)](https://www.mrlc.gov/nlcd2011.php) subset of the California Central Valley for generating training sites. 

__Question 6 (optional)__:*We have trained the classifiers using unequal samples for each class. Investigate the effect of sample size on classification. Repeat the steps with different subsets, e.g. a sample size of 100, 50, 25 per class, and compare the results. Use the same holdout samples for model evaluation.*  

